{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Cycle Annotation Mode\n",
    "\n",
    "In this notebook, we will introduce how to use the annotation mode. The annotation mode means that we combine SeqAL with third part annotation tool to run the active learning cycle.\n",
    "\n",
    "Below is the workflow that SeqAL works with annotation tool.\n",
    "\n",
    "![al_cycle_v2.png](../docs/images/al_cycle_v2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SeqAL workflow with annotation tool:\n",
    "\n",
    "- Step1: SeqAL initialize model by corpus\n",
    "- Step2: Model predicts on unlabeled data\n",
    "- Step3: SeqAL select informative data from unlabeled data according to the predictions in step2.\n",
    "- Step4: The Annotation tool get data, and annotator assign labels to the data\n",
    "- Step5: SeqAL get the annotated data and process its format\n",
    "- Step6: SeqAL add the annotated data to labeled data\n",
    "- Step7: Retrain the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We have created below datasets for research mode.\n",
    "\n",
    "- labeled data:\n",
    "    - seed data: `engtrain_seed.bio`\n",
    "    - validation data: `engtrain_dev.bio`\n",
    "    - test data: `engtest.bio`\n",
    "- unlabeled data:\n",
    "    - unlabeled data pool: `unlabeled_data_pool.txt`\n",
    "\n",
    "You can find the detail of creation process in the `data_preparation.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus\n",
    "\n",
    "We load below datasets by the following script.\n",
    "\n",
    "- seed data: `engtrain_seed.bio`\n",
    "- validation data: `engtrain_dev.bio`\n",
    "- test data: `engtest.bio`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 01:47:04,606 Reading data from ../data/ner_english_movie_simple\n",
      "2022-09-07 01:47:04,615 Train: ../data/ner_english_movie_simple/engtrain_seed.bio\n",
      "2022-09-07 01:47:04,619 Dev: ../data/ner_english_movie_simple/engtrain_dev.bio\n",
      "2022-09-07 01:47:04,621 Test: ../data/ner_english_movie_simple/engtest.bio\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "from seqal.active_learner import ActiveLearner\n",
    "from seqal.datasets import ColumnCorpus\n",
    "from seqal.samplers import LeastConfidenceSampler\n",
    "\n",
    "\n",
    "# 1. get the corpus\n",
    "columns = {0: \"text\", 1: \"ner\"}\n",
    "data_folder = \"../data/ner_english_movie_simple\"\n",
    "corpus = ColumnCorpus(\n",
    "    data_folder,\n",
    "    columns,\n",
    "    train_file=\"engtrain_seed.bio\",\n",
    "    dev_file=\"engtrain_dev.bio\",\n",
    "    test_file=\"engtest.bio\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Active Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 01:48:00,100 ----------------------------------------------------------------------------------------------------\n",
      "2022-09-07 01:48:00,102 Model: \"SequenceTagger(\n",
      "  (embeddings): WordEmbeddings(\n",
      "    'glove'\n",
      "    (embedding): Embedding(400001, 100)\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (linear): Linear(in_features=100, out_features=27, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-09-07 01:48:00,104 ----------------------------------------------------------------------------------------------------\n",
      "2022-09-07 01:48:00,105 Corpus: \"Corpus: 977 train + 977 dev + 2443 test sentences\"\n",
      "2022-09-07 01:48:00,106 ----------------------------------------------------------------------------------------------------\n",
      "2022-09-07 01:48:00,107 Parameters:\n",
      "2022-09-07 01:48:00,107  - learning_rate: \"0.1\"\n",
      "2022-09-07 01:48:00,109  - mini_batch_size: \"32\"\n",
      "2022-09-07 01:48:00,110  - patience: \"5\"\n",
      "2022-09-07 01:48:00,111  - anneal_factor: \"0.5\"\n",
      "2022-09-07 01:48:00,112  - max_epochs: \"1\"\n",
      "2022-09-07 01:48:00,113  - shuffle: \"True\"\n",
      "2022-09-07 01:48:00,114  - train_with_dev: \"False\"\n",
      "2022-09-07 01:48:00,116  - batch_growth_annealing: \"False\"\n",
      "2022-09-07 01:48:00,117 ----------------------------------------------------------------------------------------------------\n",
      "2022-09-07 01:48:00,118 Model training base path: \"output/init_train\"\n",
      "2022-09-07 01:48:00,120 ----------------------------------------------------------------------------------------------------\n",
      "2022-09-07 01:48:00,121 Device: cpu\n",
      "2022-09-07 01:48:00,122 ----------------------------------------------------------------------------------------------------\n",
      "2022-09-07 01:48:00,123 Embeddings storage mode: cpu\n",
      "2022-09-07 01:48:00,135 ----------------------------------------------------------------------------------------------------\n",
      "2022-09-07 01:48:00,280 epoch 1 - iter 3/31 - loss 2.87957323 - samples/sec: 744.91 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smap/opt/miniconda3/envs/seqal-test/lib/python3.8/site-packages/flair/trainers/trainer.py:64: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 01:48:00,358 epoch 1 - iter 6/31 - loss 2.50851027 - samples/sec: 1261.66 - lr: 0.100000\n",
      "2022-09-07 01:48:00,427 epoch 1 - iter 9/31 - loss 2.36948741 - samples/sec: 1421.70 - lr: 0.100000\n",
      "2022-09-07 01:48:00,530 epoch 1 - iter 12/31 - loss 2.24910035 - samples/sec: 939.41 - lr: 0.100000\n",
      "2022-09-07 01:48:00,603 epoch 1 - iter 15/31 - loss 2.17125771 - samples/sec: 1338.95 - lr: 0.100000\n",
      "2022-09-07 01:48:00,704 epoch 1 - iter 18/31 - loss 2.10833743 - samples/sec: 970.74 - lr: 0.100000\n",
      "2022-09-07 01:48:00,775 epoch 1 - iter 21/31 - loss 2.06120892 - samples/sec: 1360.90 - lr: 0.100000\n",
      "2022-09-07 01:48:00,852 epoch 1 - iter 24/31 - loss 2.00763254 - samples/sec: 1278.06 - lr: 0.100000\n",
      "2022-09-07 01:48:00,938 epoch 1 - iter 27/31 - loss 1.96025351 - samples/sec: 1133.14 - lr: 0.100000\n",
      "2022-09-07 01:48:01,013 epoch 1 - iter 30/31 - loss 1.92183433 - samples/sec: 1315.54 - lr: 0.100000\n",
      "2022-09-07 01:48:01,032 ----------------------------------------------------------------------------------------------------\n",
      "2022-09-07 01:48:01,033 EPOCH 1 done: loss 1.9134 - lr 0.1000000\n",
      "2022-09-07 01:48:02,710 DEV : loss 1.3541505336761475 - f1-score (micro avg)  0.0934\n",
      "2022-09-07 01:48:02,723 BAD EPOCHS (no improvement): 0\n",
      "2022-09-07 01:48:02,726 saving best model\n",
      "2022-09-07 01:48:03,885 ----------------------------------------------------------------------------------------------------\n",
      "2022-09-07 01:48:03,890 loading file output/init_train/best-model.pt\n",
      "2022-09-07 01:48:06,882 0.2983\t0.0526\t0.0895\t0.0483\n",
      "2022-09-07 01:48:06,883 \n",
      "Results:\n",
      "- F-score (micro) 0.0895\n",
      "- F-score (macro) 0.0369\n",
      "- Accuracy 0.0483\n",
      "\n",
      "By class:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          ACTOR     0.5344    0.3153    0.3966       812\n",
      "          GENRE     0.2941    0.0179    0.0338      1117\n",
      "       DIRECTOR     0.0136    0.0110    0.0121       456\n",
      "           YEAR     0.0000    0.0000    0.0000       720\n",
      "          TITLE     0.0000    0.0000    0.0000       562\n",
      "         RATING     0.0000    0.0000    0.0000       500\n",
      "           PLOT     0.0000    0.0000    0.0000       491\n",
      "RATINGS_AVERAGE     0.0000    0.0000    0.0000       451\n",
      "      CHARACTER     0.0000    0.0000    0.0000        90\n",
      "           SONG     0.0000    0.0000    0.0000        54\n",
      "         REVIEW     0.0000    0.0000    0.0000        56\n",
      "        TRAILER     0.0000    0.0000    0.0000        30\n",
      "\n",
      "      micro avg     0.2983    0.0526    0.0895      5339\n",
      "      macro avg     0.0702    0.0287    0.0369      5339\n",
      "   weighted avg     0.1440    0.0526    0.0684      5339\n",
      "    samples avg     0.0483    0.0483    0.0483      5339\n",
      "\n",
      "2022-09-07 01:48:06,886 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. tagger params\n",
    "tagger_params = {}\n",
    "tagger_params[\"tag_type\"] = \"ner\"\n",
    "tagger_params[\"hidden_size\"] = 256\n",
    "embeddings = WordEmbeddings(\"glove\")\n",
    "tagger_params[\"embeddings\"] = embeddings\n",
    "tagger_params[\"use_rnn\"] = False\n",
    "\n",
    "# 3. trainer params\n",
    "trainer_params = {}\n",
    "trainer_params[\"max_epochs\"] = 1\n",
    "trainer_params[\"mini_batch_size\"] = 32\n",
    "trainer_params[\"learning_rate\"] = 0.1\n",
    "trainer_params[\"patience\"] = 5\n",
    "\n",
    "# 4. setup active learner\n",
    "sampler = LeastConfidenceSampler()\n",
    "learner = ActiveLearner(corpus, sampler, tagger_params, trainer_params)\n",
    "\n",
    "# 5. initialize active learner\n",
    "learner.initialize(dir_path=\"output/init_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up an active learner, we have to provide `corpus`, `sampler`, `tagger_params`, and `trainer_params`. \n",
    "\n",
    "The `sampler` means the sampling method. Here we use the least confidence sampling metod (`LeastConfidenceSampler`)\n",
    "\n",
    "\n",
    "The `tagger_params` means model parameters. The default model is Bi-LSTM CRF. In order to speed up the training time, here we set the `tagger_params[\"use_rnn\"] = False`. It means that we only use the CRF model. This model is fast even in CPU.\n",
    "\n",
    "\n",
    "The `trainer_params` control the training process. We set `trainer_params[\"max_epochs\"] = 1` for demonstration. But in real case, `20` is a proper choice.\n",
    "\n",
    "\n",
    "After the setup, we can initialize the learner by calling `learner.initialize`. This will first train the model from scratch. The training log and model will be saved to `dir_path`.\n",
    "\n",
    "Related tutorial: [Active Learner Setup](../docs/TUTORIAL_3_Active_Learner_Setup.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqal.utils import load_plain_text\n",
    "\n",
    "# 6. prepare unlabeled data pool\n",
    "file_path = \"../data/ner_english_movie_simple/unlabeled_data_pool.txt\"\n",
    "unlabeled_sentences = load_plain_text(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are in the research mode, here we data pool is a labeled dataset. \n",
    "\n",
    "Related tutorial: [Prepare Data Pool](../docs/TUTORIAL_4_Prepare_Data_Pool.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. query setup\n",
    "query_number = 100\n",
    "token_based = False\n",
    "iterations = 3\n",
    "\n",
    "# initialize the tool to read annotated data\n",
    "from seqal.aligner import Aligner\n",
    "\n",
    "aligner = Aligner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `query_number` means how many data samples we want to query in each iteration. \n",
    "\n",
    "The `token_based` means we query data on sentence level or token level. If `token_based` is `True`, we will query the `100` tokens  in each iteration. If `token_based` is `False`, we will query `100` sentences in each iteration. \n",
    "\n",
    "The `iterations` means how many rounds we run the active learning cycle.\n",
    "\n",
    "\n",
    "Related tutorial: [Query Setup](../docs//TUTORIAL_6_Query_Setup.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Unlabeled Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. iteration\n",
    "for i in range(iterations):\n",
    "    # 9. query labeled sentences\n",
    "    queried_samples, unlabeled_sentences = learner.query(\n",
    "        unlabeled_sentences, query_number, token_based=token_based, research_mode=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9, the `learner.query()` run the query process. The parameter `research_mode` is `False` which means that we run a real annotation project. \n",
    "\n",
    "The `queried_samples` contains the samples selected by the sampling method. The `unlabeled_setence` contains the rest data.\n",
    "\n",
    "\n",
    "Related tutorial: [Research and Annotation Mode](../docs/TUTORIAL_5_Research_and_Annotation_Mode.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Annotated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code in one iteration.\n",
    "\n",
    "```python\n",
    "    # 10. convert sentence to plain text\n",
    "    queried_texts = [{\"text\": sent.to_plain_string()} for sent in queried_samples]\n",
    "    # queried_texts:\n",
    "    # [\n",
    "    #   {\n",
    "    #     \"text\": \"I love Berlin\"\n",
    "    #   },\n",
    "    #   {\n",
    "    #     \"text\": \"Tokyo is a city\"\n",
    "    #   }\n",
    "    # ]\n",
    "\n",
    "\n",
    "    # 11. send queried_texts to annotation tool\n",
    "    # annotator annotate the queried samples\n",
    "    # 'annotate_by_human' method should be provide by user\n",
    "    annotated_data = annotate_by_human(queried_texts)\n",
    "    # annotated_data:\n",
    "    # [\n",
    "    #     {\n",
    "    #         \"text\": ['I', 'love', 'Berlin'],\n",
    "    #         \"labels\": ['O', 'O', 'B-LOC']\n",
    "    #     }\n",
    "    #     {\n",
    "    #         \"text\": ['Tokyo', 'is', 'a', 'city'],\n",
    "    #         \"labels\": ['B-LOC', 'O', 'O', 'O']\n",
    "    #     }\n",
    "    # ]\n",
    "\n",
    "    # 12. convert data to sentence\n",
    "    queried_samples = aligner.align_spaced_language(annotated_data)\n",
    "```\n",
    "\n",
    "Step 10, we convert the queried texts to format that the annotation tool can receive. \n",
    "\n",
    "\n",
    "Step 11, the user should provide `annotate_by_human()` method, which receive the `queried_texts` to annotation tool and return the annnotation result.\n",
    "\n",
    "\n",
    "Step 12, we convert `annotated_data` to a list of `flair.data.Sentence` by `aligner`. We support different format of annotated data. More detail is in below tutorial. \n",
    "\n",
    "Related tutorial: [Annotated Data](../docs/TUTORIAL_7_Annotated_Data.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Model\n",
    "\n",
    "```python\n",
    "    # 13. retrain model, the queried_samples will be added to corpus.train\n",
    "    learner.teach(queried_samples, dir_path=f\"output/retrain_{i}\")\n",
    "```\n",
    "\n",
    "Finally, `learner.teach()` will add `queried_sampels` to the training dataset and retrain the model from scratch. The retraining log and model will be saved to `dir_path`.\n",
    "\n",
    "The whole script can be found in `examples/active_learning_cycle_annotation_mode.py`."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e065247030c9216ed33624f20c54f671a416ca6030785113c75cd1b8e233e922"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('seqal-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
