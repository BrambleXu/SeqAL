{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SeqAL SeqAL is a sequence labeling active learning framework based on Flair. Please follow the tutorial to know the usage of SeqAL. Active Learning Cycle To understand what SeqAL can do, we first introduce the pool-based active learning cycle. Step 0: Prepare seed data (a small number of labeled data used for training) Step 1: Train the model with seed data Step 2: Predict unlabeled data with the trained model Step 3: Query informative samples based on predictions Step 4: Annotator (Oracle) annotate the selected samples Step 5: Input the new labeled samples to labeled dataset Step 6: Retrain model Repeat step2~step6 until the f1 score of the model beyond the threshold or annotation budget is no left Simulation of Active Learning Cycle Run below script can run the active learning cycle. Because there is no 3rd part annotation tool, we just simulate the active learning cycle. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"dev.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings tagger_params [ \"use_rnn\" ] = False # 3. trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 # 4. setup active learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initialize active learner learner . initialize ( dir_path = \"output/init_train\" ) # 6. prepare data pool pool_file = data_folder + \"/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences # 7. query setup query_number = 2 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = True ) # 10. retrain model, the queried_samples will be added to corpus.train learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) When calling learner.query() , we set research_mode=True . This means that we simulate the active learning cycle. We prepare above code in the examples directory. If you clone SeqAL to your local machine, you should move to the root directory and run below command in terminal. python examples/active_learning_cycle_research_mode.py We also provide a notebook to demonstrate the simulation: active_learning_cycle_research_mode More detail about research (simulation) mode can be found in Research and Annotation Mode . Real Case of Active Learning Cycle Implementation In the real situation, SeqAL should connect to an annotation tool. Below code is a demonstration to show how SeqAL connect with other annotations tools. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text from seqal.aligner import Aligner from xxxx import annotate_by_human # User need to prepare this method to interact with annotation tool # 0: prepare seed data, validation data, test data, and unlabeled data pool # - labeled data: # - seed data: `train_seed.txt` # - validation data: `dev.txt` # - test data: `test.txt` # - unlabeled data: # - unlabeled data pool: `unlabeled_data_pool.txt` # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"dev.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" # what tag do we want to predict? tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings # 3. Trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"train_with_dev\" ] = True # 4. initialize learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initial training learner . initialize ( dir_path = \"output/init_train\" ) # 6. prepare unlabeled data pool file_path = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) # 7. query setup query_number = 2 token_based = False iterations = 5 # initialize the tool to read annotated data aligner = Aligner () # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. convert sentence to plain text queried_texts = [{ \"text\" : sent . to_plain_string ()} for sent in queried_samples ] # queried_texts: # [ # { # \"text\": \"I love Berlin\" # }, # { # \"text\": \"Tokyo is a city\" # } # ] # 11. send queried_texts to annotation tool # annotator annotate the queried samples # 'annotate_by_human' method should be provide by user annotated_data = annotate_by_human ( queried_texts ) # annotated_data: # [ # { # \"text\": ['I', 'love', 'Berlin'], # \"labels\": ['O', 'O', 'B-LOC'] # } # { # \"text\": ['Tokyo', 'is', 'a', 'city'], # \"labels\": ['B-LOC', 'O', 'O', 'O'] # } # ] # 12. convert data to sentence queried_samples = aligner . align_spaced_language ( annotated_data ) # 13. retrain model, the queried_samples will be added to corpus.train learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) First, we should provide the 4 files to SeqAL. The 3 labeled datasets (seed data, valid data, test data) and 1 unlabeled dataset (data pool). If you want to use the existing dataset included in Flair, you can follow the data_preparation notebook. Then you only need to provide the unlabeled dataset. If the annotation tool if the only interface that the user can interactive, the annotation tool should transfer the 4 files to SeqAL. We provide the script in examples/active_learning_cycle_annotation_mode.py and the notebook to explain in detail: active_learning_cycle_annotation_mode","title":"Home"},{"location":"#welcome-to-seqal","text":"SeqAL is a sequence labeling active learning framework based on Flair. Please follow the tutorial to know the usage of SeqAL.","title":"Welcome to SeqAL"},{"location":"#active-learning-cycle","text":"To understand what SeqAL can do, we first introduce the pool-based active learning cycle. Step 0: Prepare seed data (a small number of labeled data used for training) Step 1: Train the model with seed data Step 2: Predict unlabeled data with the trained model Step 3: Query informative samples based on predictions Step 4: Annotator (Oracle) annotate the selected samples Step 5: Input the new labeled samples to labeled dataset Step 6: Retrain model Repeat step2~step6 until the f1 score of the model beyond the threshold or annotation budget is no left","title":"Active Learning Cycle"},{"location":"#simulation-of-active-learning-cycle","text":"Run below script can run the active learning cycle. Because there is no 3rd part annotation tool, we just simulate the active learning cycle. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"dev.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings tagger_params [ \"use_rnn\" ] = False # 3. trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 # 4. setup active learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initialize active learner learner . initialize ( dir_path = \"output/init_train\" ) # 6. prepare data pool pool_file = data_folder + \"/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences # 7. query setup query_number = 2 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = True ) # 10. retrain model, the queried_samples will be added to corpus.train learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) When calling learner.query() , we set research_mode=True . This means that we simulate the active learning cycle. We prepare above code in the examples directory. If you clone SeqAL to your local machine, you should move to the root directory and run below command in terminal. python examples/active_learning_cycle_research_mode.py We also provide a notebook to demonstrate the simulation: active_learning_cycle_research_mode More detail about research (simulation) mode can be found in Research and Annotation Mode .","title":"Simulation of Active Learning Cycle"},{"location":"#real-case-of-active-learning-cycle-implementation","text":"In the real situation, SeqAL should connect to an annotation tool. Below code is a demonstration to show how SeqAL connect with other annotations tools. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text from seqal.aligner import Aligner from xxxx import annotate_by_human # User need to prepare this method to interact with annotation tool # 0: prepare seed data, validation data, test data, and unlabeled data pool # - labeled data: # - seed data: `train_seed.txt` # - validation data: `dev.txt` # - test data: `test.txt` # - unlabeled data: # - unlabeled data pool: `unlabeled_data_pool.txt` # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"dev.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" # what tag do we want to predict? tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings # 3. Trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"train_with_dev\" ] = True # 4. initialize learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initial training learner . initialize ( dir_path = \"output/init_train\" ) # 6. prepare unlabeled data pool file_path = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) # 7. query setup query_number = 2 token_based = False iterations = 5 # initialize the tool to read annotated data aligner = Aligner () # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. convert sentence to plain text queried_texts = [{ \"text\" : sent . to_plain_string ()} for sent in queried_samples ] # queried_texts: # [ # { # \"text\": \"I love Berlin\" # }, # { # \"text\": \"Tokyo is a city\" # } # ] # 11. send queried_texts to annotation tool # annotator annotate the queried samples # 'annotate_by_human' method should be provide by user annotated_data = annotate_by_human ( queried_texts ) # annotated_data: # [ # { # \"text\": ['I', 'love', 'Berlin'], # \"labels\": ['O', 'O', 'B-LOC'] # } # { # \"text\": ['Tokyo', 'is', 'a', 'city'], # \"labels\": ['B-LOC', 'O', 'O', 'O'] # } # ] # 12. convert data to sentence queried_samples = aligner . align_spaced_language ( annotated_data ) # 13. retrain model, the queried_samples will be added to corpus.train learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) First, we should provide the 4 files to SeqAL. The 3 labeled datasets (seed data, valid data, test data) and 1 unlabeled dataset (data pool). If you want to use the existing dataset included in Flair, you can follow the data_preparation notebook. Then you only need to provide the unlabeled dataset. If the annotation tool if the only interface that the user can interactive, the annotation tool should transfer the 4 files to SeqAL. We provide the script in examples/active_learning_cycle_annotation_mode.py and the notebook to explain in detail: active_learning_cycle_annotation_mode","title":"Real Case of Active Learning Cycle Implementation"},{"location":"TUTORIAL_10_Performance_Recorder/","text":"Performance Recorder This tutorial shows how to use performance recorder. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags , count_tokens from seqal.performance_recorder import PerformanceRecorder # 1~6 steps can be found in Introduction # 7. query setup query_percent = 0.02 token_based = True total_tokens = count_tokens ( corpus . train . sentences ) + count_tokens ( data_pool . sentences ) query_number = tokens_each_iteration = int ( total_tokens * query_percent ) # performance recorder setup performance_recorder = PerformanceRecorder () accumulate_data = 0 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = add_tags ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) # 12. performance recorder get result result = learner . trained_tagger . evaluate ( corpus . test , gold_label_type = \"ner\" ) accumulate_data += query_percent performance_recorder . get_result ( accumulate_data , result ) iteration_performance = performance_recorder . performance_list [ i ] print ( iteration_performance . data ) print ( iteration_performance . precision ) print ( iteration_performance . recall ) print ( iteration_performance . accuracy ) print ( iteration_performance . micro_f1 ) print ( iteration_performance . macro_f1 ) print ( iteration_performance . weighted_f1 ) print ( iteration_performance . samples_f1 ) print ( iteration_performance . label_scores ) performance_recorder . save ( \"lc_performance.txt\" ) performance_recorder . plot ( metric = \"micro_f1\" , sampling_method = \"lc\" , save_path = \"lc_performance.jpg\" ) As the above shows, we use performance_recorder to record the evaluation result of all iterations to the performance_list property. For each iteration_performance , we could get the scores by accessing properties. Finally, we can save the performance by calling performance_recorder.save() and plot the graph by calling performance_recorder.plot() . The metric specify the score we want to draw, the sampling_metod will show up on the graph legend, and the save_path will save the graph to the image.","title":"Performance Recorder"},{"location":"TUTORIAL_10_Performance_Recorder/#performance-recorder","text":"This tutorial shows how to use performance recorder. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags , count_tokens from seqal.performance_recorder import PerformanceRecorder # 1~6 steps can be found in Introduction # 7. query setup query_percent = 0.02 token_based = True total_tokens = count_tokens ( corpus . train . sentences ) + count_tokens ( data_pool . sentences ) query_number = tokens_each_iteration = int ( total_tokens * query_percent ) # performance recorder setup performance_recorder = PerformanceRecorder () accumulate_data = 0 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = add_tags ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) # 12. performance recorder get result result = learner . trained_tagger . evaluate ( corpus . test , gold_label_type = \"ner\" ) accumulate_data += query_percent performance_recorder . get_result ( accumulate_data , result ) iteration_performance = performance_recorder . performance_list [ i ] print ( iteration_performance . data ) print ( iteration_performance . precision ) print ( iteration_performance . recall ) print ( iteration_performance . accuracy ) print ( iteration_performance . micro_f1 ) print ( iteration_performance . macro_f1 ) print ( iteration_performance . weighted_f1 ) print ( iteration_performance . samples_f1 ) print ( iteration_performance . label_scores ) performance_recorder . save ( \"lc_performance.txt\" ) performance_recorder . plot ( metric = \"micro_f1\" , sampling_method = \"lc\" , save_path = \"lc_performance.jpg\" ) As the above shows, we use performance_recorder to record the evaluation result of all iterations to the performance_list property. For each iteration_performance , we could get the scores by accessing properties. Finally, we can save the performance by calling performance_recorder.save() and plot the graph by calling performance_recorder.plot() . The metric specify the score we want to draw, the sampling_metod will show up on the graph legend, and the save_path will save the graph to the image.","title":"Performance Recorder"},{"location":"TUTORIAL_11_Multiple_Language_Support/","text":"Multiple Language Support This tutorial shows how to use SeqAL for different language. Introduction shows example on English. If you want to use SeqAL on other language, you need to meet the following requirements. Prepare the data with BIO or BIOES format on the language you used. More detail can be found in Prepare Corpus Prepare the embedding on the same language. We can use TransformerWordEmbeddings to read different languages' embedding from HuggingFace . If the language of dataset is a kind of non-spaced language, we have to use spacy model to tokenize the dataset. We introduce the processing workflow for spaced language and non-spaced language below. Spaced Language Below is the simple example of active learning cycle on English. from seqal.datasets import ColumnCorpus from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.samplers import LeastConfidenceSampler ## 1 Load data columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) # Change to the dataset on the language you used ## 2 Initialize Active Learner tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) # Prepare the embedding on the same language tagger_params [ \"embeddings\" ] = embeddings trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) ## 3 Active Learning Cycle query_number = 200 for i in range ( 5 ): unlabeled_sentences , queried_samples = learner . query ( unlabeled_sentences , query_number , token_based = False ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) If you want to change the language, you just need change few lines. Below is an example on German. First, changing the dataset on the language that you used. data_folder = \"./data/conll_deu\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train.txt\" , test_file = \"test.txt\" , dev_file = \"dev.txt\" , ) Next, changing the embedding with the same language. embedding = WordEmbeddings ( \"de\" ) Check out the full list of all word embeddings models here , along with more explanations on the WordEmbeddings class. We also can use the contextualized embedding. from flair.embeddings import BertEmbeddings embedding = BertEmbeddings ( \"bert-base-german-cased\" ) Here are more explanations on the TransformerWordEmbeddings class. We can find more language embeddings on Non-spaced Language Prepare Corpus First we provide the corpus with CoNLL format like below: \u6771\u4eac B-LOC \u306f O \u90fd\u5e02 O \u3067\u3059 O Then we load the corpus just like the spaced language. columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_jp\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) You can see the examples in ./data/sample_jp directory. Prepare Embeddings We provide the proper embedding for specified language. Below is a Japances example. from flair.embeddings import StackedEmbeddings , FlairEmbeddings embedding_types = [ FlairEmbeddings ( 'ja-forward' ), FlairEmbeddings ( 'ja-backward' ), ] embeddings = StackedEmbeddings ( embeddings = embedding_types ) Prepare Data Pool Research Mode If we just want to simulate the active learning cycle (the research mode), we can should load the labeled data pool by ColumnDataset and set the research_mode as True . from seqal.datasets import ColumnDataset columns = { 0 : \"text\" , 1 : \"ner\" } pool_file = \"./data/sample_jp/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences # ... for i in range ( iterations ): queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = True ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) Annotation Mode If we are dealing with a real annotation project, we usually load the data from plain text. file_path = \"./data/sample_jp/unlabeled_data_pool.txt\" unlabeled_sentences = [] with open ( file_path , mode = \"r\" , encoding = \"utf-8\" ) as f : for line in f : unlabeled_sentences . append ( line ) Because the loaded sentence is non-spaced, we have to tokenize the sentence. For examples, we tokenize the \u6771\u4eac\u306f\u90fd\u5e02\u3067\u3059 to [\"\u6771\u4eac\", \"\u306f\", \"\u90fd\u5e02\", \"\u3067\u3059\"] . There are two ways to tokenize the sentences. The first method is using the Transformer.to_subword() . import spacy from seqal.transformer import Transformer nlp = spacy . load ( \"ja_core_news_sm\" ) tokenizer = Transformer ( nlp ) unlabeled_sentences = [ tokenizer . to_subword ( sentence ) for sentence in unlabeled_sentences ] The second method is directly using the spacy tokenizer. import spacy from flair.data import Sentence from flair.tokenization import SpacyTokenizer tokenizer = SpacyTokenizer ( \"ja_core_news_sm\" ) unlabeled_sentences = [ Sentence ( sentence , use_tokenizer = tokenizer ) for sentence in sentences ] And do not forget set the research_mode as False . for i in range ( iterations ): queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) Run Active Learning Cycle Executing below command will run the active learning cycle on non-spaced language. The user can see the script for more detail. python examples/active_learning_cycle_research_mode_non_spaced_language.py","title":"Multiple Language"},{"location":"TUTORIAL_11_Multiple_Language_Support/#multiple-language-support","text":"This tutorial shows how to use SeqAL for different language. Introduction shows example on English. If you want to use SeqAL on other language, you need to meet the following requirements. Prepare the data with BIO or BIOES format on the language you used. More detail can be found in Prepare Corpus Prepare the embedding on the same language. We can use TransformerWordEmbeddings to read different languages' embedding from HuggingFace . If the language of dataset is a kind of non-spaced language, we have to use spacy model to tokenize the dataset. We introduce the processing workflow for spaced language and non-spaced language below.","title":"Multiple Language Support"},{"location":"TUTORIAL_11_Multiple_Language_Support/#spaced-language","text":"Below is the simple example of active learning cycle on English. from seqal.datasets import ColumnCorpus from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.samplers import LeastConfidenceSampler ## 1 Load data columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) # Change to the dataset on the language you used ## 2 Initialize Active Learner tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) # Prepare the embedding on the same language tagger_params [ \"embeddings\" ] = embeddings trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) ## 3 Active Learning Cycle query_number = 200 for i in range ( 5 ): unlabeled_sentences , queried_samples = learner . query ( unlabeled_sentences , query_number , token_based = False ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) If you want to change the language, you just need change few lines. Below is an example on German. First, changing the dataset on the language that you used. data_folder = \"./data/conll_deu\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train.txt\" , test_file = \"test.txt\" , dev_file = \"dev.txt\" , ) Next, changing the embedding with the same language. embedding = WordEmbeddings ( \"de\" ) Check out the full list of all word embeddings models here , along with more explanations on the WordEmbeddings class. We also can use the contextualized embedding. from flair.embeddings import BertEmbeddings embedding = BertEmbeddings ( \"bert-base-german-cased\" ) Here are more explanations on the TransformerWordEmbeddings class. We can find more language embeddings on","title":"Spaced Language"},{"location":"TUTORIAL_11_Multiple_Language_Support/#non-spaced-language","text":"","title":"Non-spaced Language"},{"location":"TUTORIAL_11_Multiple_Language_Support/#prepare-corpus","text":"First we provide the corpus with CoNLL format like below: \u6771\u4eac B-LOC \u306f O \u90fd\u5e02 O \u3067\u3059 O Then we load the corpus just like the spaced language. columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_jp\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) You can see the examples in ./data/sample_jp directory.","title":"Prepare Corpus"},{"location":"TUTORIAL_11_Multiple_Language_Support/#prepare-embeddings","text":"We provide the proper embedding for specified language. Below is a Japances example. from flair.embeddings import StackedEmbeddings , FlairEmbeddings embedding_types = [ FlairEmbeddings ( 'ja-forward' ), FlairEmbeddings ( 'ja-backward' ), ] embeddings = StackedEmbeddings ( embeddings = embedding_types )","title":"Prepare Embeddings"},{"location":"TUTORIAL_11_Multiple_Language_Support/#prepare-data-pool","text":"Research Mode If we just want to simulate the active learning cycle (the research mode), we can should load the labeled data pool by ColumnDataset and set the research_mode as True . from seqal.datasets import ColumnDataset columns = { 0 : \"text\" , 1 : \"ner\" } pool_file = \"./data/sample_jp/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences # ... for i in range ( iterations ): queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = True ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) Annotation Mode If we are dealing with a real annotation project, we usually load the data from plain text. file_path = \"./data/sample_jp/unlabeled_data_pool.txt\" unlabeled_sentences = [] with open ( file_path , mode = \"r\" , encoding = \"utf-8\" ) as f : for line in f : unlabeled_sentences . append ( line ) Because the loaded sentence is non-spaced, we have to tokenize the sentence. For examples, we tokenize the \u6771\u4eac\u306f\u90fd\u5e02\u3067\u3059 to [\"\u6771\u4eac\", \"\u306f\", \"\u90fd\u5e02\", \"\u3067\u3059\"] . There are two ways to tokenize the sentences. The first method is using the Transformer.to_subword() . import spacy from seqal.transformer import Transformer nlp = spacy . load ( \"ja_core_news_sm\" ) tokenizer = Transformer ( nlp ) unlabeled_sentences = [ tokenizer . to_subword ( sentence ) for sentence in unlabeled_sentences ] The second method is directly using the spacy tokenizer. import spacy from flair.data import Sentence from flair.tokenization import SpacyTokenizer tokenizer = SpacyTokenizer ( \"ja_core_news_sm\" ) unlabeled_sentences = [ Sentence ( sentence , use_tokenizer = tokenizer ) for sentence in sentences ] And do not forget set the research_mode as False . for i in range ( iterations ): queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" )","title":"Prepare Data Pool"},{"location":"TUTORIAL_11_Multiple_Language_Support/#run-active-learning-cycle","text":"Executing below command will run the active learning cycle on non-spaced language. The user can see the script for more detail. python examples/active_learning_cycle_research_mode_non_spaced_language.py","title":"Run Active Learning Cycle"},{"location":"TUTORIAL_1_Introduction/","text":"Introduction This tutorial shows how to use SeqAL to perform active learning cycle for NER(named entity recognition). We will use the below example to explain the SeqAL usage with third part annotation tool. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text from seqal.aligner import Aligner from xxxx import annotate_by_human # User need to prepare this method to interact with annotation tool # 0: prepare seed data, validation data, test data, and unlabeled data pool # - labeled data: # - seed data: `train_seed.txt` # - validation data: `dev.txt` # - test data: `test.txt` # - unlabeled data: # - unlabeled data pool: `unlabeled_data_pool.txt` # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"dev.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" # what tag do we want to predict? tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings # 3. Trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 10 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"train_with_dev\" ] = True # 4. initialize learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initial training learner . initialize ( dir_path = \"output/init_train\" ) # 6. prepare unlabeled data pool file_path = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) # 7. query setup query_number = 2 token_based = False iterations = 5 # initialize the tool to read annotated data aligner = Aligner () # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. convert sentence to plain text queried_texts = [{ \"text\" : sent . to_plain_string ()} for sent in queried_samples ] # 11. send queried_texts to annotation tool # annotator annotate the queried samples # 'annotate_by_human' method should be provide by user annotated_data = annotate_by_human ( queried_texts ) # 12. convert data to sentence queried_samples = aligner . align_spaced_language ( annotated_data ) # 13. retrain model, the queried_samples will be added to corpus.train learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) 1 Load Corpus First, we need to prepare small labeled data (seed data), which can be split into training data, validation data, and test data. columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/conll\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) For such data, we import the ColumnCorpus class and provide a columns variable to specify which column is the name entity tag. The train_seed.txt is the dataset used to train the model. The dev.txt is the dataset used to give an estimate of model skill while tuning the model\u2019s hyperparameters. The test.txt is the dataset used to give an unbiased estimate of the final tuned model. Related tutorial: Prepare Corpus 2~5 Initialize Active Learner # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings # 3. trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 # 4. setup active learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initialize active learner learner . initialize ( dir_path = \"output/init_train\" ) To set up an active learner, we have to provide corpus , sampler , tagger_params , and trainer_params . The sampler means the sampling method. The tagger_params means model parameters. The default model is Bi-LSTM CRF. The trainer_params control the training process. After the setup, we can initialize the learner by calling learner.initialize . This will first train the model from scratch. The training log and model will be saved to dir_path . Related tutorial: Active Learner Setup 6 Prepare Data Pool # 6. prepare data pool file_path = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) The data pool should contain unlabeled data. We can load the plain text (one sentence one line) by load_plain_text . This will return a list of flair.data.Sentence . Related tutorial: Prepare Data Pool 7 Query Setup # 7. query setup token_based = False query_number = 10 iterations = 5 The query_number means how many data samples we want to query in each iteration. The token_based means we query data on sentence level or token level. If token_based is True , we will query the 10 tokens in each iteration. If token_based is False , we will query 10 sentences in each iteration. The iterations means how many rounds we run the active learning cycle. Related tutorial: 6 Query Setup 8~9 Query Unlabeled Data # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) Step 9, the learner.query() run the query process. The parameter research_mode is False which means that we run a real annotation project. The detail can be found in Research and Annotation Mode . The queried_samples contains the samples selected by the sampling method. The unlabeled_setence contains the rest data. Related tutorial: Research and Annotation Mode 10~12 Get Annotated Data # 10. convert sentence to plain text queried_texts = [{ \"text\" : sent . to_plain_string ()} for sent in queried_samples ] # queried_texts: # [ # { # \"text\": \"I love Berlin\" # }, # { # \"text\": \"Tokyo is a city\" # } # ] # 11. send queried_texts to annotation tool # annotator annotate the queried samples # 'annotate_by_human' method should be provide by user annotated_data = annotate_by_human ( queried_texts ) # annotated_data: # [ # { # \"text\": ['I', 'love', 'Berlin'], # \"labels\": ['O', 'O', 'B-LOC'] # } # { # \"text\": ['Tokyo', 'is', 'a', 'city'], # \"labels\": ['B-LOC', 'O', 'O', 'O'] # } # ] # 12. convert data to sentence queried_samples = aligner . align_spaced_language ( annotated_data ) Step 10, we convert the queried texts to format that the annotation tool can receive. Step 11, the user should provide annotate_by_human() method, which receive the queried_texts to annotation tool and return the annnotation result. Step 12, we convert annotated_data to a list of flair.data.Sentence by aligner . We support different format of annotated data. More detail is in below tutorial. Related tutorial: Annotated Data 13 Retrain Model # 13. retrain model, the queried_samples will be added to corpus.train learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) Finally, learner.teach() will add queried_sampels to the training dataset and retrain the model from scratch. In each iteration, the model will print the performance on different labels, like below: Results: - F-score (micro) 0.6969 - F-score (macro) 0.6603 - Accuracy 0.5495 By class: precision recall f1-score support PER 0.8441 0.7934 0.8180 1617 LOC 0.8431 0.6151 0.7113 1668 ORG 0.7852 0.5105 0.6188 1661 MISC 0.7943 0.3575 0.4931 702 micro avg 0.8246 0.6034 0.6969 5648 macro avg 0.8167 0.5692 0.6603 5648 weighted avg 0.8203 0.6034 0.6875 5648 samples avg 0.5495 0.5495 0.5495 5648","title":"Introduction"},{"location":"TUTORIAL_1_Introduction/#introduction","text":"This tutorial shows how to use SeqAL to perform active learning cycle for NER(named entity recognition). We will use the below example to explain the SeqAL usage with third part annotation tool. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text from seqal.aligner import Aligner from xxxx import annotate_by_human # User need to prepare this method to interact with annotation tool # 0: prepare seed data, validation data, test data, and unlabeled data pool # - labeled data: # - seed data: `train_seed.txt` # - validation data: `dev.txt` # - test data: `test.txt` # - unlabeled data: # - unlabeled data pool: `unlabeled_data_pool.txt` # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"dev.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" # what tag do we want to predict? tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings # 3. Trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 10 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"train_with_dev\" ] = True # 4. initialize learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initial training learner . initialize ( dir_path = \"output/init_train\" ) # 6. prepare unlabeled data pool file_path = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) # 7. query setup query_number = 2 token_based = False iterations = 5 # initialize the tool to read annotated data aligner = Aligner () # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. convert sentence to plain text queried_texts = [{ \"text\" : sent . to_plain_string ()} for sent in queried_samples ] # 11. send queried_texts to annotation tool # annotator annotate the queried samples # 'annotate_by_human' method should be provide by user annotated_data = annotate_by_human ( queried_texts ) # 12. convert data to sentence queried_samples = aligner . align_spaced_language ( annotated_data ) # 13. retrain model, the queried_samples will be added to corpus.train learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" )","title":"Introduction"},{"location":"TUTORIAL_1_Introduction/#1-load-corpus","text":"First, we need to prepare small labeled data (seed data), which can be split into training data, validation data, and test data. columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/conll\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) For such data, we import the ColumnCorpus class and provide a columns variable to specify which column is the name entity tag. The train_seed.txt is the dataset used to train the model. The dev.txt is the dataset used to give an estimate of model skill while tuning the model\u2019s hyperparameters. The test.txt is the dataset used to give an unbiased estimate of the final tuned model. Related tutorial: Prepare Corpus","title":"1 Load Corpus"},{"location":"TUTORIAL_1_Introduction/#25-initialize-active-learner","text":"# 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings # 3. trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 # 4. setup active learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initialize active learner learner . initialize ( dir_path = \"output/init_train\" ) To set up an active learner, we have to provide corpus , sampler , tagger_params , and trainer_params . The sampler means the sampling method. The tagger_params means model parameters. The default model is Bi-LSTM CRF. The trainer_params control the training process. After the setup, we can initialize the learner by calling learner.initialize . This will first train the model from scratch. The training log and model will be saved to dir_path . Related tutorial: Active Learner Setup","title":"2~5 Initialize Active Learner"},{"location":"TUTORIAL_1_Introduction/#6-prepare-data-pool","text":"# 6. prepare data pool file_path = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) The data pool should contain unlabeled data. We can load the plain text (one sentence one line) by load_plain_text . This will return a list of flair.data.Sentence . Related tutorial: Prepare Data Pool","title":"6 Prepare Data Pool"},{"location":"TUTORIAL_1_Introduction/#7-query-setup","text":"# 7. query setup token_based = False query_number = 10 iterations = 5 The query_number means how many data samples we want to query in each iteration. The token_based means we query data on sentence level or token level. If token_based is True , we will query the 10 tokens in each iteration. If token_based is False , we will query 10 sentences in each iteration. The iterations means how many rounds we run the active learning cycle. Related tutorial: 6 Query Setup","title":"7 Query Setup"},{"location":"TUTORIAL_1_Introduction/#89-query-unlabeled-data","text":"# 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) Step 9, the learner.query() run the query process. The parameter research_mode is False which means that we run a real annotation project. The detail can be found in Research and Annotation Mode . The queried_samples contains the samples selected by the sampling method. The unlabeled_setence contains the rest data. Related tutorial: Research and Annotation Mode","title":"8~9 Query Unlabeled Data"},{"location":"TUTORIAL_1_Introduction/#1012-get-annotated-data","text":"# 10. convert sentence to plain text queried_texts = [{ \"text\" : sent . to_plain_string ()} for sent in queried_samples ] # queried_texts: # [ # { # \"text\": \"I love Berlin\" # }, # { # \"text\": \"Tokyo is a city\" # } # ] # 11. send queried_texts to annotation tool # annotator annotate the queried samples # 'annotate_by_human' method should be provide by user annotated_data = annotate_by_human ( queried_texts ) # annotated_data: # [ # { # \"text\": ['I', 'love', 'Berlin'], # \"labels\": ['O', 'O', 'B-LOC'] # } # { # \"text\": ['Tokyo', 'is', 'a', 'city'], # \"labels\": ['B-LOC', 'O', 'O', 'O'] # } # ] # 12. convert data to sentence queried_samples = aligner . align_spaced_language ( annotated_data ) Step 10, we convert the queried texts to format that the annotation tool can receive. Step 11, the user should provide annotate_by_human() method, which receive the queried_texts to annotation tool and return the annnotation result. Step 12, we convert annotated_data to a list of flair.data.Sentence by aligner . We support different format of annotated data. More detail is in below tutorial. Related tutorial: Annotated Data","title":"10~12 Get Annotated Data"},{"location":"TUTORIAL_1_Introduction/#13-retrain-model","text":"# 13. retrain model, the queried_samples will be added to corpus.train learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) Finally, learner.teach() will add queried_sampels to the training dataset and retrain the model from scratch. In each iteration, the model will print the performance on different labels, like below: Results: - F-score (micro) 0.6969 - F-score (macro) 0.6603 - Accuracy 0.5495 By class: precision recall f1-score support PER 0.8441 0.7934 0.8180 1617 LOC 0.8431 0.6151 0.7113 1668 ORG 0.7852 0.5105 0.6188 1661 MISC 0.7943 0.3575 0.4931 702 micro avg 0.8246 0.6034 0.6969 5648 macro avg 0.8167 0.5692 0.6603 5648 weighted avg 0.8203 0.6034 0.6875 5648 samples avg 0.5495 0.5495 0.5495 5648","title":"13 Retrain Model"},{"location":"TUTORIAL_2_Prepare_Corpus/","text":"Prepare Corpus This tutorial shows how to prepare corpus. We can load the custom dataset by below script. from seqal.datasets import ColumnCorpus # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) If we want to use the existing corpus in flair datasets , there are introductions in data_preparation notebook. Data format Flair support Flair supports the BIO schema and the BIOES schema . So we need our data to follow the BIO schema or BIOES schema. If you want to change to BIO shema or BIOES shema, we provide the below methods. from seqal import utils bilou_tags = [ \"B-X\" , \"I-X\" , \"L-X\" , \"U-X\" , \"O\" ] bioes_tags = utils . bilou2bio ( bilou_tags ) bioes_tags = utils . bio2bioes ( bio_tags ) bio_tags = utils . bilou2bio ( bilou_tags ) bio_tags = utils . bioes2bio ( bioes_tags ) Spaced Language The spaced language means a sentence can split tokens by space, like English ( \"Tokyo is a city\" ) and Spanish ( \"Tokio es una ciudad\" ). An example with BIO format: Alex I-PER is O going O to O Los I-LOC Angeles I-LOC in O California I-LOC An example with BIOES format: Alex S-PER is O going O with O Marty B-PER A. I-PER Rick E-PER to O Los B-LOC Angeles E-LOC Non-spaced Language The non-spaced language means a sentence can not be split by space, like Japanese ( \"\u6771\u4eac\u306f\u90fd\u5e02\u3067\u3059\" ) and Chinese ( \"\u4e1c\u4eac\u662f\u90fd\u5e02\" ). Usually, one character with a label. \u6771 B-LOC \u4eac I-LOC \u306f O \u90fd O \u5e02 O \u3067 O \u3059 O But this format cannot be trained by flair. So we have to tokenize the sentence and merge the tags like below. An example with BIO format: \u6771\u4eac B-LOC \u306f O \u90fd\u5e02 O \u3067\u3059 O An example with BIOES format: \u6771\u4eac S-LOC \u306f O \u90fd\u5e02 O \u3067\u3059 O Corpus Usage We can access different dataset by below commands. # print the number of Sentences in the train split print ( len ( corpus . train )) # print the number of Sentences in the test split print ( len ( corpus . test )) # print the number of Sentences in the dev split print ( len ( corpus . dev )) We can access one sentence in each dataset. # print the one Sentence in the training dataset print ( corpus . train [ 19 ]) This prints: Sentence: \"Germany imported 47,600 sheep from Britain last year , nearly half of total imports .\" [\u2212 Tokens: 15 \u2212 Token-Labels: \"Germany <B-LOC> imported 47,600 sheep from Britain <B-LOC> last year , nearly half of total imports .\"] ``` This sentence contains NER tags. We can print it with NER tags. ```python print(corpus.train[19].to_tagged_string('ner')) This prints: Germany <B-LOC> imported 47,600 sheep from Britain <B-LOC> last year , nearly half of total imports . We can get labels from one sentence. sentence = corpus . train [ 19 ] for entity in sentence . get_spans ( 'ner' ): print ( entity . text , entity . tag ) This prints: Germany LOC Britain LOC We also can get label of each token. for token in sentence : tag = token . get_tag ( 'ner' ) print ( token . text , tag . value , tag . score ) This prints: Germany B-LOC 1.0 imported O 1.0 47,600 O 1.0 sheep O 1.0 from O 1.0 Britain B-LOC 1.0 last O 1.0 year O 1.0 , O 1.0 nearly O 1.0 half O 1.0 of O 1.0 total O 1.0 imports O 1.0 . O 1.0 The score is confidence score. Because we read the entities' labels from dataset, it assumes that the labels are glod annotations. The confidence score of glod annotaiotns is 1.0. If a sentence is predicted by model, the condidence score should be lower than 1.0. Below is an example that use a pre-trained model to predict a sentence. from flair.models import SequenceTagger tagger = SequenceTagger . load ( 'ner' ) sentence = Sentence ( 'George Washington went to Washington.' ) tagger . predict ( sentence ) for token in sentence : tag = token . get_tag ( 'ner' ) print ( token . text , tag . value , tag . score ) It prints: George B-PER 0.9978131055831909 Washington E-PER 0.9999594688415527 went O 0.999995231628418 to O 0.9999998807907104 Washington S-LOC 0.9942096471786499 . O 0.99989914894104 The seqal.datasets.ColumnCorpus inherit from flair.data.Corpus . We recommend the flair tutorials for more detail. Related tutorials: - Tutorial 1: Basics - Tutorial 2: Tagging your Text - Tutorial 6: Loading a Dataset","title":"Prepare Corpus"},{"location":"TUTORIAL_2_Prepare_Corpus/#prepare-corpus","text":"This tutorial shows how to prepare corpus. We can load the custom dataset by below script. from seqal.datasets import ColumnCorpus # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) If we want to use the existing corpus in flair datasets , there are introductions in data_preparation notebook.","title":"Prepare Corpus"},{"location":"TUTORIAL_2_Prepare_Corpus/#data-format","text":"Flair support Flair supports the BIO schema and the BIOES schema . So we need our data to follow the BIO schema or BIOES schema. If you want to change to BIO shema or BIOES shema, we provide the below methods. from seqal import utils bilou_tags = [ \"B-X\" , \"I-X\" , \"L-X\" , \"U-X\" , \"O\" ] bioes_tags = utils . bilou2bio ( bilou_tags ) bioes_tags = utils . bio2bioes ( bio_tags ) bio_tags = utils . bilou2bio ( bilou_tags ) bio_tags = utils . bioes2bio ( bioes_tags )","title":"Data format"},{"location":"TUTORIAL_2_Prepare_Corpus/#spaced-language","text":"The spaced language means a sentence can split tokens by space, like English ( \"Tokyo is a city\" ) and Spanish ( \"Tokio es una ciudad\" ). An example with BIO format: Alex I-PER is O going O to O Los I-LOC Angeles I-LOC in O California I-LOC An example with BIOES format: Alex S-PER is O going O with O Marty B-PER A. I-PER Rick E-PER to O Los B-LOC Angeles E-LOC","title":"Spaced Language"},{"location":"TUTORIAL_2_Prepare_Corpus/#non-spaced-language","text":"The non-spaced language means a sentence can not be split by space, like Japanese ( \"\u6771\u4eac\u306f\u90fd\u5e02\u3067\u3059\" ) and Chinese ( \"\u4e1c\u4eac\u662f\u90fd\u5e02\" ). Usually, one character with a label. \u6771 B-LOC \u4eac I-LOC \u306f O \u90fd O \u5e02 O \u3067 O \u3059 O But this format cannot be trained by flair. So we have to tokenize the sentence and merge the tags like below. An example with BIO format: \u6771\u4eac B-LOC \u306f O \u90fd\u5e02 O \u3067\u3059 O An example with BIOES format: \u6771\u4eac S-LOC \u306f O \u90fd\u5e02 O \u3067\u3059 O","title":"Non-spaced Language"},{"location":"TUTORIAL_2_Prepare_Corpus/#corpus-usage","text":"We can access different dataset by below commands. # print the number of Sentences in the train split print ( len ( corpus . train )) # print the number of Sentences in the test split print ( len ( corpus . test )) # print the number of Sentences in the dev split print ( len ( corpus . dev )) We can access one sentence in each dataset. # print the one Sentence in the training dataset print ( corpus . train [ 19 ]) This prints: Sentence: \"Germany imported 47,600 sheep from Britain last year , nearly half of total imports .\" [\u2212 Tokens: 15 \u2212 Token-Labels: \"Germany <B-LOC> imported 47,600 sheep from Britain <B-LOC> last year , nearly half of total imports .\"] ``` This sentence contains NER tags. We can print it with NER tags. ```python print(corpus.train[19].to_tagged_string('ner')) This prints: Germany <B-LOC> imported 47,600 sheep from Britain <B-LOC> last year , nearly half of total imports . We can get labels from one sentence. sentence = corpus . train [ 19 ] for entity in sentence . get_spans ( 'ner' ): print ( entity . text , entity . tag ) This prints: Germany LOC Britain LOC We also can get label of each token. for token in sentence : tag = token . get_tag ( 'ner' ) print ( token . text , tag . value , tag . score ) This prints: Germany B-LOC 1.0 imported O 1.0 47,600 O 1.0 sheep O 1.0 from O 1.0 Britain B-LOC 1.0 last O 1.0 year O 1.0 , O 1.0 nearly O 1.0 half O 1.0 of O 1.0 total O 1.0 imports O 1.0 . O 1.0 The score is confidence score. Because we read the entities' labels from dataset, it assumes that the labels are glod annotations. The confidence score of glod annotaiotns is 1.0. If a sentence is predicted by model, the condidence score should be lower than 1.0. Below is an example that use a pre-trained model to predict a sentence. from flair.models import SequenceTagger tagger = SequenceTagger . load ( 'ner' ) sentence = Sentence ( 'George Washington went to Washington.' ) tagger . predict ( sentence ) for token in sentence : tag = token . get_tag ( 'ner' ) print ( token . text , tag . value , tag . score ) It prints: George B-PER 0.9978131055831909 Washington E-PER 0.9999594688415527 went O 0.999995231628418 to O 0.9999998807907104 Washington S-LOC 0.9942096471786499 . O 0.99989914894104 The seqal.datasets.ColumnCorpus inherit from flair.data.Corpus . We recommend the flair tutorials for more detail. Related tutorials: - Tutorial 1: Basics - Tutorial 2: Tagging your Text - Tutorial 6: Loading a Dataset","title":"Corpus Usage"},{"location":"TUTORIAL_3_Active_Learner_Setup/","text":"Active Learner Setup This tutorial shows how so setup Active Learner. Below is the simple example to setup the ActiveLearner . from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings tagger_params [ \"use_rnn\" ] = False # 3. trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 # 4. setup active learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) To setup active learner, we have to provide corpus , sampler , tagger_params , and trainer_params . We introduce sampler , tagger_params and trainer_params below. Detail about corpus in TUTORIAL_2_Prepare_Corpus Sampler The seqal.samplers provides below sampling methods. Uncertainty based sampling method: LeastConfidenceSampler (Least Confidence; LC) MaxNormLogProbSampler (Maximum Normalized Log-Probability; MNLP) Diversity based sampling method: StringNGramSampler (String N-Gram Similarity) DistributeSimilaritySampler (Distribute Similarity; DS) ClusterSimilaritySampler (Cluster Similarity; CS) Combine uncertainty and diversity sampling method: CombinedMultipleSampler : LC+DS, LC+CS, MNLP+DS, MNLP+CS Other: RandomSampler : Random sampling method According to our experiment (Japanese), there are some advice to choose a suitable sampling method. - If you want to decrease training time, we recommend the uncertainty-based sampling methods - If you want to increase the performance, we recommend the combined sampled methods. Below are some setup method for different samples. from sklearn.preprocessing import MinMaxScaler from seqal.samplers import ( ClusterSimilaritySampler , CombinedMultipleSampler , DistributeSimilaritySampler , LeastConfidenceSampler , MaxNormLogProbSampler , StringNGramSampler RandomSampler , ) # RandomSampler setup random_sampler = RandomSampler () # LeastConfidenceSampler setup lc_sampler = LeastConfidenceSampler () # MaxNormLogProbSampler setup mnlp_sampler = MaxNormLogProbSampler () # StringNGramSampler setup, n=3 sn_sampler = StringNGramSampler () # DistributeSimilaritySampler setup ds_sampler = DistributeSimilaritySampler () # ClusterSimilaritySampler setup kmeans_params = { \"n_clusters\" : 2 , \"n_init\" : 10 , \"random_state\" : 0 } cs_sampler = ClusterSimilaritySampler ( kmeans_params ) # CombinedMultipleSampler setup sampler_type = \"lc_cs\" combined_type = \"parallel\" kmeans_params = { \"n_clusters\" : 8 , \"n_init\" : 10 , \"random_state\" : 0 } scaler = MinMaxScaler () cm_sampler = CombinedMultipleSampler ( sampler_type = sampler_type , combined_type = combined_type , kmeans_params = kmeans_params , scaler = scaler ) Most of samples' setup is simple. The biggest difference are the setups of ClusterSimilaritySampler() and CombinedMultipleSampler() . The ClusterSimilaritySampler() needs parameter for kmeans. One impartant thing is the number of n_clusters should be the same with the label types, except \"O\". More parameters detail can be found in sklearn.cluster.KMeans . The parameters of CombinedMultipleSampler() : sampler_type : Samples to use. Defaults to \"lc_ds\". Available types are \"lc_ds\", \"lc_cs\", \"mnlp_ds\", \"mnlp_cs\" lc_ds : LeastConfidenceSampler and DistributeSimilaritySampler lc_cs : LeastConfidenceSampler and ClusterSimilaritySampler mnlp_ds : MaxNormLogProbSampler and DistributeSimilaritySampler mnlp_cs : MaxNormLogProbSampler and ClusterSimilaritySampler combined_type : The combined method of different samplers parallel : run two samplers together series : run one sampler first and then run the second sampler kmeans_params : Parameters for clustering. When sampler_type contains cs , we need to add kmeans parameters. More parameters on sklearn.cluster.KMeans scaler : The scaler method for two kinds of samplers. When combined_type is parallel , scaler will normalize the scores of two kinds of samplers. More scaler can be found in sklearn.preprocessing Tagger parameters Because we use the flair model, you can find more detail about parameters in flair.SequenceTagger GPU model (Bi-LSTM CRF) tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings The tagger_params means model parameters. The default model is Bi-LSTM CRF. tag_type : what kind of tag we want to predict, like 'ner', 'pos', and so on. hidden_szie : number of hidden states in RNN embeddings : word embedding used in tagger. Make sure the dataset and embeddings are the same language. CPU model (CRF) If we want to speed up the training cycle, we can just use the CRF model by add below setup. tagger_params [ \"use_rnn\" ] = False According to the comparing result of the GPU model and CPU model, we highly recommend to use the CPU model. The performance of the GPU model is slightly better than the performance of the CPU model, but the CPU model's training speed is far faster than the GPU model's. And the price of a CPU machine is only about half the price of a GPU machine. Trainer parameters trainer_params = {} trainer_params [ \"max_epochs\" ] = 10 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 The trainer_params control the training process. max_epochs : the maximum number of epochs to train in each iteration. Usually, we set this value smaller than 20 to decrease the training time. mini_batch_size : minimum size of data samples in each batch. learning_rate : initial learning rate. patience : the number of epochs with no improvement the Trainer waits. Because we use the flair model, you can find more detail about parameters in flair.ModelTrainer.train","title":"Setup Active Learner"},{"location":"TUTORIAL_3_Active_Learner_Setup/#active-learner-setup","text":"This tutorial shows how so setup Active Learner. Below is the simple example to setup the ActiveLearner . from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings tagger_params [ \"use_rnn\" ] = False # 3. trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 # 4. setup active learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) To setup active learner, we have to provide corpus , sampler , tagger_params , and trainer_params . We introduce sampler , tagger_params and trainer_params below. Detail about corpus in TUTORIAL_2_Prepare_Corpus","title":"Active Learner Setup"},{"location":"TUTORIAL_3_Active_Learner_Setup/#sampler","text":"The seqal.samplers provides below sampling methods. Uncertainty based sampling method: LeastConfidenceSampler (Least Confidence; LC) MaxNormLogProbSampler (Maximum Normalized Log-Probability; MNLP) Diversity based sampling method: StringNGramSampler (String N-Gram Similarity) DistributeSimilaritySampler (Distribute Similarity; DS) ClusterSimilaritySampler (Cluster Similarity; CS) Combine uncertainty and diversity sampling method: CombinedMultipleSampler : LC+DS, LC+CS, MNLP+DS, MNLP+CS Other: RandomSampler : Random sampling method According to our experiment (Japanese), there are some advice to choose a suitable sampling method. - If you want to decrease training time, we recommend the uncertainty-based sampling methods - If you want to increase the performance, we recommend the combined sampled methods. Below are some setup method for different samples. from sklearn.preprocessing import MinMaxScaler from seqal.samplers import ( ClusterSimilaritySampler , CombinedMultipleSampler , DistributeSimilaritySampler , LeastConfidenceSampler , MaxNormLogProbSampler , StringNGramSampler RandomSampler , ) # RandomSampler setup random_sampler = RandomSampler () # LeastConfidenceSampler setup lc_sampler = LeastConfidenceSampler () # MaxNormLogProbSampler setup mnlp_sampler = MaxNormLogProbSampler () # StringNGramSampler setup, n=3 sn_sampler = StringNGramSampler () # DistributeSimilaritySampler setup ds_sampler = DistributeSimilaritySampler () # ClusterSimilaritySampler setup kmeans_params = { \"n_clusters\" : 2 , \"n_init\" : 10 , \"random_state\" : 0 } cs_sampler = ClusterSimilaritySampler ( kmeans_params ) # CombinedMultipleSampler setup sampler_type = \"lc_cs\" combined_type = \"parallel\" kmeans_params = { \"n_clusters\" : 8 , \"n_init\" : 10 , \"random_state\" : 0 } scaler = MinMaxScaler () cm_sampler = CombinedMultipleSampler ( sampler_type = sampler_type , combined_type = combined_type , kmeans_params = kmeans_params , scaler = scaler ) Most of samples' setup is simple. The biggest difference are the setups of ClusterSimilaritySampler() and CombinedMultipleSampler() . The ClusterSimilaritySampler() needs parameter for kmeans. One impartant thing is the number of n_clusters should be the same with the label types, except \"O\". More parameters detail can be found in sklearn.cluster.KMeans . The parameters of CombinedMultipleSampler() : sampler_type : Samples to use. Defaults to \"lc_ds\". Available types are \"lc_ds\", \"lc_cs\", \"mnlp_ds\", \"mnlp_cs\" lc_ds : LeastConfidenceSampler and DistributeSimilaritySampler lc_cs : LeastConfidenceSampler and ClusterSimilaritySampler mnlp_ds : MaxNormLogProbSampler and DistributeSimilaritySampler mnlp_cs : MaxNormLogProbSampler and ClusterSimilaritySampler combined_type : The combined method of different samplers parallel : run two samplers together series : run one sampler first and then run the second sampler kmeans_params : Parameters for clustering. When sampler_type contains cs , we need to add kmeans parameters. More parameters on sklearn.cluster.KMeans scaler : The scaler method for two kinds of samplers. When combined_type is parallel , scaler will normalize the scores of two kinds of samplers. More scaler can be found in sklearn.preprocessing","title":"Sampler"},{"location":"TUTORIAL_3_Active_Learner_Setup/#tagger-parameters","text":"Because we use the flair model, you can find more detail about parameters in flair.SequenceTagger","title":"Tagger parameters"},{"location":"TUTORIAL_3_Active_Learner_Setup/#gpu-model-bi-lstm-crf","text":"tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings The tagger_params means model parameters. The default model is Bi-LSTM CRF. tag_type : what kind of tag we want to predict, like 'ner', 'pos', and so on. hidden_szie : number of hidden states in RNN embeddings : word embedding used in tagger. Make sure the dataset and embeddings are the same language.","title":"GPU model (Bi-LSTM CRF)"},{"location":"TUTORIAL_3_Active_Learner_Setup/#cpu-model-crf","text":"If we want to speed up the training cycle, we can just use the CRF model by add below setup. tagger_params [ \"use_rnn\" ] = False According to the comparing result of the GPU model and CPU model, we highly recommend to use the CPU model. The performance of the GPU model is slightly better than the performance of the CPU model, but the CPU model's training speed is far faster than the GPU model's. And the price of a CPU machine is only about half the price of a GPU machine.","title":"CPU model (CRF)"},{"location":"TUTORIAL_3_Active_Learner_Setup/#trainer-parameters","text":"trainer_params = {} trainer_params [ \"max_epochs\" ] = 10 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 The trainer_params control the training process. max_epochs : the maximum number of epochs to train in each iteration. Usually, we set this value smaller than 20 to decrease the training time. mini_batch_size : minimum size of data samples in each batch. learning_rate : initial learning rate. patience : the number of epochs with no improvement the Trainer waits. Because we use the flair model, you can find more detail about parameters in flair.ModelTrainer.train","title":"Trainer parameters"},{"location":"TUTORIAL_4_Prepare_Data_Pool/","text":"Prepare Data Pool This tutorial shows how to prepare data pool. Load CoNLL Format We can use the ColumnDataset class to load CoNLL format. from seqal.datasets import ColumnDataset columns = { 0 : \"text\" , 1 : \"ner\" } pool_file = \"./data/sample_bio/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences We can get sentences from data_pool by calling sentences property. print ( unlabeled_sentences [ 0 ]) This prints: Sentence: \"this is New York\" [\u2212 Tokens: 4 \u2212 Token-Labels: \"this is New <B-LOC> York <I-LOC>\"] Load Plain Text We can use load_plain_text to read the unlabeled dataset. This will create a list of Sentence objects. from seqal.utils import load_plain_text file_path = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) print ( unlabeled_sentences [ 0 ]) This prints: Sentence: \"this is New York\" [\u2212 Tokens: 4] Non-spaced Language As we mentioned in TUTORIAL_2_Prepare_Corpus , we have to provide the tokenized data for non-spaced language. An example with CoNLL format: \u6771\u4eac B-LOC \u306f O \u90fd\u5e02 O \u3067\u3059 O If the input format is plain text \u6771\u4eac\u306f\u90fd\u5e02\u3067\u3059 . We should tokenize the sentence. We mainly use the spacy model for the tokenization. import spacy from seqal.transformer import Transformer nlp = spacy . load ( \"ja_core_news_sm\" ) tokenizer = Transformer ( nlp ) unlabeled_sentences = [ tokenizer . to_subword ( sentence ) for sentence in sentences ] We also can directly use the spacy tokenizer. import spacy from flair.data import Sentence from flair.tokenization import SpacyTokenizer tokenizer = SpacyTokenizer ( \"ja_core_news_sm\" ) unlabeled_sentences = [ Sentence ( sentence , use_tokenizer = tokenizer ) for sentence in sentences ] We should download the spacy model beforehand and we can find different language's spacy model in spacy models .","title":"Prepare Data Pool"},{"location":"TUTORIAL_4_Prepare_Data_Pool/#prepare-data-pool","text":"This tutorial shows how to prepare data pool.","title":"Prepare Data Pool"},{"location":"TUTORIAL_4_Prepare_Data_Pool/#load-conll-format","text":"We can use the ColumnDataset class to load CoNLL format. from seqal.datasets import ColumnDataset columns = { 0 : \"text\" , 1 : \"ner\" } pool_file = \"./data/sample_bio/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences We can get sentences from data_pool by calling sentences property. print ( unlabeled_sentences [ 0 ]) This prints: Sentence: \"this is New York\" [\u2212 Tokens: 4 \u2212 Token-Labels: \"this is New <B-LOC> York <I-LOC>\"]","title":"Load CoNLL Format"},{"location":"TUTORIAL_4_Prepare_Data_Pool/#load-plain-text","text":"We can use load_plain_text to read the unlabeled dataset. This will create a list of Sentence objects. from seqal.utils import load_plain_text file_path = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) print ( unlabeled_sentences [ 0 ]) This prints: Sentence: \"this is New York\" [\u2212 Tokens: 4]","title":"Load Plain Text"},{"location":"TUTORIAL_4_Prepare_Data_Pool/#non-spaced-language","text":"As we mentioned in TUTORIAL_2_Prepare_Corpus , we have to provide the tokenized data for non-spaced language. An example with CoNLL format: \u6771\u4eac B-LOC \u306f O \u90fd\u5e02 O \u3067\u3059 O If the input format is plain text \u6771\u4eac\u306f\u90fd\u5e02\u3067\u3059 . We should tokenize the sentence. We mainly use the spacy model for the tokenization. import spacy from seqal.transformer import Transformer nlp = spacy . load ( \"ja_core_news_sm\" ) tokenizer = Transformer ( nlp ) unlabeled_sentences = [ tokenizer . to_subword ( sentence ) for sentence in sentences ] We also can directly use the spacy tokenizer. import spacy from flair.data import Sentence from flair.tokenization import SpacyTokenizer tokenizer = SpacyTokenizer ( \"ja_core_news_sm\" ) unlabeled_sentences = [ Sentence ( sentence , use_tokenizer = tokenizer ) for sentence in sentences ] We should download the spacy model beforehand and we can find different language's spacy model in spacy models .","title":"Non-spaced Language"},{"location":"TUTORIAL_5_Research_and_Annotation_Mode/","text":"Research and Annotation Mode This tutorial shows explain what are research mode and annotation mode. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings tagger_params [ \"use_rnn\" ] = False # 3. trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 # 4. setup active learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initialize active learner learner . initialize ( dir_path = \"output/init_train\" ) # 6. prepare data pool pool_file = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) # 7. query setup query_number = 10 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) The parameter research_mode controls which mode we use. Research mode The research mode means we run the experiment for research purposes. When we are doing research, we already have a labeled dataset. So we do not need people to annotate the data. We just want to simulate the active learning cycle to see the performance of the model. When the model predicts, predicted labels will overwrite the gold labels. But we assume that humans will assign glod labels. In the case of adding predicted labels to the training dataset, we set the research_mode as True . Make sure that we load the labeled data pool. from seqal.datasets import ColumnDataset # 1~5 steps can be found in Introduction # 6. prepare data pool from conll format columns = { 0 : \"text\" , 1 : \"ner\" } pool_file = \"./data/sample_bio/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences # 7. query setup query_number = 10 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = True ) Annotation mode The annotation mode means we use SeqAL in a real annotation project, which means that the data pool does not contain labels. We set the research_mode as False . from seqal.datasets import ColumnDataset from seqal.utils import load_plain_text # 6. prepare data pool from conll format columns = { 0 : \"text\" } pool_file = \"./data/sample_bio/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences # 6. prepare data pool from plain text pool_file = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) # 7. query setup query_number = 10 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False )","title":"Research and Annotation Mode"},{"location":"TUTORIAL_5_Research_and_Annotation_Mode/#research-and-annotation-mode","text":"This tutorial shows explain what are research mode and annotation mode. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags # 1. get the corpus columns = { 0 : \"text\" , 1 : \"ner\" } data_folder = \"./data/sample_bio\" corpus = ColumnCorpus ( data_folder , columns , train_file = \"train_seed.txt\" , dev_file = \"valid.txt\" , test_file = \"test.txt\" , ) # 2. tagger params tagger_params = {} tagger_params [ \"tag_type\" ] = \"ner\" tagger_params [ \"hidden_size\" ] = 256 embeddings = WordEmbeddings ( \"glove\" ) tagger_params [ \"embeddings\" ] = embeddings tagger_params [ \"use_rnn\" ] = False # 3. trainer params trainer_params = {} trainer_params [ \"max_epochs\" ] = 1 trainer_params [ \"mini_batch_size\" ] = 32 trainer_params [ \"learning_rate\" ] = 0.1 trainer_params [ \"patience\" ] = 5 # 4. setup active learner sampler = LeastConfidenceSampler () learner = ActiveLearner ( corpus , sampler , tagger_params , trainer_params ) # 5. initialize active learner learner . initialize ( dir_path = \"output/init_train\" ) # 6. prepare data pool pool_file = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) # 7. query setup query_number = 10 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) The parameter research_mode controls which mode we use.","title":"Research and Annotation Mode"},{"location":"TUTORIAL_5_Research_and_Annotation_Mode/#research-mode","text":"The research mode means we run the experiment for research purposes. When we are doing research, we already have a labeled dataset. So we do not need people to annotate the data. We just want to simulate the active learning cycle to see the performance of the model. When the model predicts, predicted labels will overwrite the gold labels. But we assume that humans will assign glod labels. In the case of adding predicted labels to the training dataset, we set the research_mode as True . Make sure that we load the labeled data pool. from seqal.datasets import ColumnDataset # 1~5 steps can be found in Introduction # 6. prepare data pool from conll format columns = { 0 : \"text\" , 1 : \"ner\" } pool_file = \"./data/sample_bio/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences # 7. query setup query_number = 10 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = True )","title":"Research mode"},{"location":"TUTORIAL_5_Research_and_Annotation_Mode/#annotation-mode","text":"The annotation mode means we use SeqAL in a real annotation project, which means that the data pool does not contain labels. We set the research_mode as False . from seqal.datasets import ColumnDataset from seqal.utils import load_plain_text # 6. prepare data pool from conll format columns = { 0 : \"text\" } pool_file = \"./data/sample_bio/labeled_data_pool.txt\" data_pool = ColumnDataset ( pool_file , columns ) unlabeled_sentences = data_pool . sentences # 6. prepare data pool from plain text pool_file = \"./data/sample_bio/unlabeled_data_pool.txt\" unlabeled_sentences = load_plain_text ( file_path ) # 7. query setup query_number = 10 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False )","title":"Annotation mode"},{"location":"TUTORIAL_6_Query_Setup/","text":"Query Setup This tutorial shows how to make reasonable query. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags # 1~6 steps can be found in Introduction # 7. query setup query_number = 10 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) The query setup above is to query 10 sentences in each iterations. Usually we will make more 'smarter' query setup. Query on Sentence If we set token_based as False , this will count query_number as a sentence number. We prefer to give a percentage query number to query data instead of a fixed query number. # 7. query setup query_percent = 0.02 token_based = False total_sentences = len ( corpus . train . sentences ) + len ( data_pool . sentences ) query_number = int ( total_sentences * query_percent ) # queried sentences in each iteration # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) The query_percent could be 0.01 or 0.02 . Query on Token If we set token_based as True , this will count query_number as a token number. In the real case, we usually query data based on the token. Because we don't know how many tokens a queried sentence has. from seqal.utils import count_tokens # 7. query setup query_percent = 0.02 token_based = True total_tokens = count_tokens ( corpus . train . sentences ) + count_tokens ( data_pool . sentences ) query_number = tokens_each_iteration = int ( total_tokens * query_percent ) # queried tokens in each iteration # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False )","title":"Setup Query"},{"location":"TUTORIAL_6_Query_Setup/#query-setup","text":"This tutorial shows how to make reasonable query. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags # 1~6 steps can be found in Introduction # 7. query setup query_number = 10 token_based = False iterations = 5 # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) The query setup above is to query 10 sentences in each iterations. Usually we will make more 'smarter' query setup.","title":"Query Setup"},{"location":"TUTORIAL_6_Query_Setup/#query-on-sentence","text":"If we set token_based as False , this will count query_number as a sentence number. We prefer to give a percentage query number to query data instead of a fixed query number. # 7. query setup query_percent = 0.02 token_based = False total_sentences = len ( corpus . train . sentences ) + len ( data_pool . sentences ) query_number = int ( total_sentences * query_percent ) # queried sentences in each iteration # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) The query_percent could be 0.01 or 0.02 .","title":"Query on Sentence"},{"location":"TUTORIAL_6_Query_Setup/#query-on-token","text":"If we set token_based as True , this will count query_number as a token number. In the real case, we usually query data based on the token. Because we don't know how many tokens a queried sentence has. from seqal.utils import count_tokens # 7. query setup query_percent = 0.02 token_based = True total_tokens = count_tokens ( corpus . train . sentences ) + count_tokens ( data_pool . sentences ) query_number = tokens_each_iteration = int ( total_tokens * query_percent ) # queried tokens in each iteration # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False )","title":"Query on Token"},{"location":"TUTORIAL_7_Annotated_Data/","text":"Annotated Data This tutorial shows how to receive the labeled data. The annotated data could have 4 kinds of format. Next we will demonstrate how to process different annotated data. Token-based Annotated Data Token-based annotated data means one token has one label. Example of spaced language: [ { \"text\": ['Tokyo', 'is', 'a', 'city'], \"labels\": ['B-LOC', 'O', 'O', 'O'] } ] Example of non-spaced language: [ { \"text\": ['\u6771\u4eac', '\u306f', '\u90fd\u5e02', '\u3067\u3059'], \"labels\": ['B-LOC', 'O', 'O', 'O'] } ] For such annotated data, we can use Aligner.add_tags_on_token() to process it. # 1~7 steps can be found in Introduction # initialize Aligner aligner = Aligner () # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = aligner . add_tags_on_token ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) In step 10, it receive the annotated data. In step 11, Aligner.add_tags_on_token() convert annotated data to the format that could be added to training data. The detail of spaced language and non-spaced language can be found in TUTORIAL_2_Prepare_Corpus . Character-based Annotated Data Token-based annotated data means one character has one label. Character-based Spaced Language Example of character-based spaced language. [ { \"text\": ['T', 'o', 'k', 'y', 'o', ' ', 'c', 'i', 't', 'y'], \"labels\": [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\", 'O', 'O', 'O', 'O', 'O'] }, ] For such annotated data, we can use Aligner.add_tags_on_char_spaced_language() to process it. # 1~7 steps can be found in Introduction # initialize Aligner aligner = Aligner () # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = aligner . add_tags_on_char_spaced_language ( annotated_data , input_schema = \"BIO\" , output_schema = \"BIO\" , tag_type = \"ner\" ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) In step 11, Aligner.add_tags_on_char_spaced_language() convert annotated data to the format that could be added to training data. The parameters: - annotated_data : A list of labeled data. - input_schema : The schema of input tags. It supports \"BIO\", \"BILOU\", \"BIOES\". - output_schema : The schema of output tags. It supports \"BIO\", \"BIOES\". Because Flair only support \"BIO\" and \"BIOES\", we output these two kind of schema. - tag_type : The tag type. It can be \"ner\", \"pos\" and so on. Character-based Non-spaced Language Example of character-based non-spaced language. [ { \"text\": ['\u6771', '\u4eac', '\u306f', '\u90fd', '\u5e02', '\u3067', '\u3059'], \"labels\": [\"B-LOC\", \"I-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\"] } ] For such annotated data, we can use Aligner.add_tags_on_char_non_spaced_language() to process it. # 1~7 steps can be found in Introduction # initialize Aligner aligner = Aligner () nlp = spacy . load ( \"ja_core_news_sm\" ) # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = aligner . add_tags_on_char_non_spaced_language ( annotated_data , input_schema = \"BIO\" , output_schema = \"BIO\" , tag_type = \"ner\" , spacy_model = nlp ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) Different from spaced language, we have to tokenize non-spaced language. For example, we tokenize \"\u30ed\u30f3\u30c9\u30f3\u306f\u90fd\u5e02\u3067\u3059\" to [\"\u30ed\u30f3\u30c9\u30f3\"\u306f\", \"\u90fd\u5e02\", \"\u3067\u3059\"] . We should download the spacy model beforehand as the tokenizer. We can find different language's spacy model in spacy models . The parameters: - annotated_data : A list of labeled data. - input_schema : The schema of input tags. It supports \"BIO\", \"BILOU\", \"BIOES\". - output_schema : The schema of output tags. It supports \"BIO\", \"BIOES\". Because Flair only support \"BIO\" and \"BIOES\", we output these two kind of schema. - tag_type : The tag type. It can be \"ner\", \"pos\" and so on. - spacy_model : The spacy language model.","title":"Annotated Data"},{"location":"TUTORIAL_7_Annotated_Data/#annotated-data","text":"This tutorial shows how to receive the labeled data. The annotated data could have 4 kinds of format. Next we will demonstrate how to process different annotated data.","title":"Annotated Data"},{"location":"TUTORIAL_7_Annotated_Data/#token-based-annotated-data","text":"Token-based annotated data means one token has one label. Example of spaced language: [ { \"text\": ['Tokyo', 'is', 'a', 'city'], \"labels\": ['B-LOC', 'O', 'O', 'O'] } ] Example of non-spaced language: [ { \"text\": ['\u6771\u4eac', '\u306f', '\u90fd\u5e02', '\u3067\u3059'], \"labels\": ['B-LOC', 'O', 'O', 'O'] } ] For such annotated data, we can use Aligner.add_tags_on_token() to process it. # 1~7 steps can be found in Introduction # initialize Aligner aligner = Aligner () # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = aligner . add_tags_on_token ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) In step 10, it receive the annotated data. In step 11, Aligner.add_tags_on_token() convert annotated data to the format that could be added to training data. The detail of spaced language and non-spaced language can be found in TUTORIAL_2_Prepare_Corpus .","title":"Token-based Annotated Data"},{"location":"TUTORIAL_7_Annotated_Data/#character-based-annotated-data","text":"Token-based annotated data means one character has one label.","title":"Character-based Annotated Data"},{"location":"TUTORIAL_7_Annotated_Data/#character-based-spaced-language","text":"Example of character-based spaced language. [ { \"text\": ['T', 'o', 'k', 'y', 'o', ' ', 'c', 'i', 't', 'y'], \"labels\": [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\", 'O', 'O', 'O', 'O', 'O'] }, ] For such annotated data, we can use Aligner.add_tags_on_char_spaced_language() to process it. # 1~7 steps can be found in Introduction # initialize Aligner aligner = Aligner () # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = aligner . add_tags_on_char_spaced_language ( annotated_data , input_schema = \"BIO\" , output_schema = \"BIO\" , tag_type = \"ner\" ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) In step 11, Aligner.add_tags_on_char_spaced_language() convert annotated data to the format that could be added to training data. The parameters: - annotated_data : A list of labeled data. - input_schema : The schema of input tags. It supports \"BIO\", \"BILOU\", \"BIOES\". - output_schema : The schema of output tags. It supports \"BIO\", \"BIOES\". Because Flair only support \"BIO\" and \"BIOES\", we output these two kind of schema. - tag_type : The tag type. It can be \"ner\", \"pos\" and so on.","title":"Character-based Spaced Language"},{"location":"TUTORIAL_7_Annotated_Data/#character-based-non-spaced-language","text":"Example of character-based non-spaced language. [ { \"text\": ['\u6771', '\u4eac', '\u306f', '\u90fd', '\u5e02', '\u3067', '\u3059'], \"labels\": [\"B-LOC\", \"I-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\"] } ] For such annotated data, we can use Aligner.add_tags_on_char_non_spaced_language() to process it. # 1~7 steps can be found in Introduction # initialize Aligner aligner = Aligner () nlp = spacy . load ( \"ja_core_news_sm\" ) # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = aligner . add_tags_on_char_non_spaced_language ( annotated_data , input_schema = \"BIO\" , output_schema = \"BIO\" , tag_type = \"ner\" , spacy_model = nlp ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) Different from spaced language, we have to tokenize non-spaced language. For example, we tokenize \"\u30ed\u30f3\u30c9\u30f3\u306f\u90fd\u5e02\u3067\u3059\" to [\"\u30ed\u30f3\u30c9\u30f3\"\u306f\", \"\u90fd\u5e02\", \"\u3067\u3059\"] . We should download the spacy model beforehand as the tokenizer. We can find different language's spacy model in spacy models . The parameters: - annotated_data : A list of labeled data. - input_schema : The schema of input tags. It supports \"BIO\", \"BILOU\", \"BIOES\". - output_schema : The schema of output tags. It supports \"BIO\", \"BIOES\". Because Flair only support \"BIO\" and \"BIOES\", we output these two kind of schema. - tag_type : The tag type. It can be \"ner\", \"pos\" and so on. - spacy_model : The spacy language model.","title":"Character-based Non-spaced Language"},{"location":"TUTORIAL_8_Stopper/","text":"Stopper This tutorial shows how to use stoppers. Stop by Budget Annotation costs a lot of money. Usually, we will have a budget. If we run out of budget, the active learning cycle will stop. Below is a demo to show how to use BudgetStopper . from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags , count_tokens from seqal.stoppers import BudgetStopper # 1~7 # Stopper setup stopper = BudgetStopper ( goal = 200 , unit_price = 0.02 ) # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = add_tags ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) # 12. stop iteration early unit_count = count_tokens ( corpus . train . sentences ) if stopper . stop ( unit_count ): break The BudgetStopper(goal=200, unit_price=0.02) initialize the budget stopper. The goal means how much money we have, here we say 200\\$. The unit_price means annotation cost for each unit, here we say 0.02\\$/unit. A unit could be a sentence or a token. Usually, it is a token. Stop by Metric Another motivation to stop active learning cycle is model's performance is beyond our goal. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags , count_tokens from seqal.stoppers import MetricStopper # 1~6 steps can be found in Introduction # 7. query setup query_percent = 0.02 token_based = True total_tokens = count_tokens ( corpus . train . sentences ) + count_tokens ( data_pool . sentences ) query_number = tokens_each_iteration = int ( total_tokens * query_percent ) # performance recorder setup performance_recorder = PerformanceRecorder () accumulate_data = 0 # Stopper setup stopper = MetricStopper ( goal = 0.9 ) # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = add_tags ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) # 12. stop iteration early result = learner . trained_tagger . evaluate ( corpus . test , gold_label_type = \"ner\" ) accumulate_data += query_percent performance_recorder . get_result ( accumulate_data , result ) iteration_performance = performance_recorder . performance_list [ i ] if stopper . stop ( iteration_performance . micro_f1 ): break The MetricStopper(goal=0.9) initialize the budget stopper. The goal means the f1 score we want to achieve. The learner.trained_tagger.evaluate(corpus.test, gold_label_type=\"ner\") evaluate on test dataset and return the evaluation result. We use performance_recorder to parse the evaluation result. The stopper.stop(iteration_performance.micro_f1) compare the goal and the evaluation result on micro f1 score. We can also compare other metrics like macro f1, accuracy, etc.","title":"Stopper"},{"location":"TUTORIAL_8_Stopper/#stopper","text":"This tutorial shows how to use stoppers.","title":"Stopper"},{"location":"TUTORIAL_8_Stopper/#stop-by-budget","text":"Annotation costs a lot of money. Usually, we will have a budget. If we run out of budget, the active learning cycle will stop. Below is a demo to show how to use BudgetStopper . from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags , count_tokens from seqal.stoppers import BudgetStopper # 1~7 # Stopper setup stopper = BudgetStopper ( goal = 200 , unit_price = 0.02 ) # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = add_tags ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) # 12. stop iteration early unit_count = count_tokens ( corpus . train . sentences ) if stopper . stop ( unit_count ): break The BudgetStopper(goal=200, unit_price=0.02) initialize the budget stopper. The goal means how much money we have, here we say 200\\$. The unit_price means annotation cost for each unit, here we say 0.02\\$/unit. A unit could be a sentence or a token. Usually, it is a token.","title":"Stop by Budget"},{"location":"TUTORIAL_8_Stopper/#stop-by-metric","text":"Another motivation to stop active learning cycle is model's performance is beyond our goal. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags , count_tokens from seqal.stoppers import MetricStopper # 1~6 steps can be found in Introduction # 7. query setup query_percent = 0.02 token_based = True total_tokens = count_tokens ( corpus . train . sentences ) + count_tokens ( data_pool . sentences ) query_number = tokens_each_iteration = int ( total_tokens * query_percent ) # performance recorder setup performance_recorder = PerformanceRecorder () accumulate_data = 0 # Stopper setup stopper = MetricStopper ( goal = 0.9 ) # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = add_tags ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) # 12. stop iteration early result = learner . trained_tagger . evaluate ( corpus . test , gold_label_type = \"ner\" ) accumulate_data += query_percent performance_recorder . get_result ( accumulate_data , result ) iteration_performance = performance_recorder . performance_list [ i ] if stopper . stop ( iteration_performance . micro_f1 ): break The MetricStopper(goal=0.9) initialize the budget stopper. The goal means the f1 score we want to achieve. The learner.trained_tagger.evaluate(corpus.test, gold_label_type=\"ner\") evaluate on test dataset and return the evaluation result. We use performance_recorder to parse the evaluation result. The stopper.stop(iteration_performance.micro_f1) compare the goal and the evaluation result on micro f1 score. We can also compare other metrics like macro f1, accuracy, etc.","title":"Stop by Metric"},{"location":"TUTORIAL_9_Output_Labeled_Data/","text":"Output Labeled Data This tutorial shows how to output labeled data. After annotation we need to get the labeled data. We can output it on CoNLL format or JSON format. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags , output_labeled_data # 1~7 steps can be found in Introduction # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = add_tags ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) # 12. Output labeled data output_labeled_data ( corpus . train . sentences , file_path = \"labeled_data.txt\" , file_format = \"conll\" , tag_type = 'ner' ) We should know that all newly labeled data are added to the training dataset, so we just need to output the training dataset. The file_path is the path to save the file, file_format is the output format, and the tag_type is the tag type we want to output. If we want to output the JSON file, we change the use below code. output_labeled_data ( corpus . train . sentences , file_path = \"labeled_data.json\" , file_format = \"json\" , tag_type = 'ner' )","title":"Output Labeled Data"},{"location":"TUTORIAL_9_Output_Labeled_Data/#output-labeled-data","text":"This tutorial shows how to output labeled data. After annotation we need to get the labeled data. We can output it on CoNLL format or JSON format. from flair.embeddings import WordEmbeddings from seqal.active_learner import ActiveLearner from seqal.datasets import ColumnCorpus , ColumnDataset from seqal.samplers import LeastConfidenceSampler from seqal.utils import load_plain_text , add_tags , output_labeled_data # 1~7 steps can be found in Introduction # 8. iteration for i in range ( iterations ): # 9. query unlabeled sentences queried_samples , unlabeled_sentences = learner . query ( unlabeled_sentences , query_number , token_based = token_based , research_mode = False ) # 10. annotate data annotated_data = human_annotate ( queried_samples ) # 11. retrain model with newly added queried_samples queried_samples = add_tags ( annotated_data ) learner . teach ( queried_samples , dir_path = f \"output/retrain_ { i } \" ) # 12. Output labeled data output_labeled_data ( corpus . train . sentences , file_path = \"labeled_data.txt\" , file_format = \"conll\" , tag_type = 'ner' ) We should know that all newly labeled data are added to the training dataset, so we just need to output the training dataset. The file_path is the path to save the file, file_format is the output format, and the tag_type is the tag type we want to output. If we want to output the JSON file, we change the use below code. output_labeled_data ( corpus . train . sentences , file_path = \"labeled_data.json\" , file_format = \"json\" , tag_type = 'ner' )","title":"Output Labeled Data"},{"location":"installation/","text":"Installation Installation from PyPI SeqAL is available on PyPI: pip install seqal SeqAL officially supports Python 3.8+. Construct Envirement Locally If you want to make a PR or implement something locally, you can follow below instruction to construct the development envirement locally. It will install the latest SeqAL from the main branch. We use conda as the envirement management tool, so install it first. Here is the installation tutorial for conda . We recommend the install Miniconda due to it's small size. First we create a environment seqal based on the environment.yml file. conda env create -f environment.yml Then we activate the environment. conda activate seqal Install poetry for dependency management. curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - Add poetry path in your shell configure file ( bashrc , zshrc , etc.) export PATH=\"$HOME/.poetry/bin:$PATH\" Installing dependencies from pyproject.toml . poetry install This command will install all dependencies to seqal environment. You can make development locally now. If you want to delete the local envirement, run below command. conda remove --name seqal --all","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#installation-from-pypi","text":"SeqAL is available on PyPI: pip install seqal SeqAL officially supports Python 3.8+.","title":"Installation from PyPI"},{"location":"installation/#construct-envirement-locally","text":"If you want to make a PR or implement something locally, you can follow below instruction to construct the development envirement locally. It will install the latest SeqAL from the main branch. We use conda as the envirement management tool, so install it first. Here is the installation tutorial for conda . We recommend the install Miniconda due to it's small size. First we create a environment seqal based on the environment.yml file. conda env create -f environment.yml Then we activate the environment. conda activate seqal Install poetry for dependency management. curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - Add poetry path in your shell configure file ( bashrc , zshrc , etc.) export PATH=\"$HOME/.poetry/bin:$PATH\" Installing dependencies from pyproject.toml . poetry install This command will install all dependencies to seqal environment. You can make development locally now. If you want to delete the local envirement, run below command. conda remove --name seqal --all","title":"Construct Envirement Locally"},{"location":"performance/","text":"SeqAL Performance on Metrics and Time Cost Performance We will show the performance of SeqAL on different datasets with different language. Supervise means that training model on full data LC (Least confidence) and MNLP (Maximum Normalized Log-Probability) are query algorithm with different calculation on informativeness. Random means randomly query data without caring about informativeness. CS (Clustering Similarity) and DS (Distributed Similarity) are the diversity based sampling methods for NER We first train the model on seed data (2% of training data). Then we query 2\uff05 of training data in each iteration until 50% data are queried. SeqAL on different languages Ontonotes 5.0 (EN) Setup: - dataset: CoNLL 2003(English) - model: Bi-LSTM CRF - total query data: 50% - epochs: 20 - batch_size: 32 - learning_rate: 0.015 - embeddings: bert-base-uncased - GPU: AWS p3.8xlarge BCCWJ (JP) Setup: - dataset: BCCWJ - model: Bi-LSTM CRF - total query data: 50% - epochs: 20 - batch_size: 32 - learning_rate: 0.015 - embeddings: bert-base-japanese - GPU: AWS p3.8xlarge People's Daily 2014 (CN) Setup: - dataset: People's Daily 2014 - model: Bi-LSTM CRF - total query data: 50% - epochs: 20 - batch_size: 32 - learning_rate: 0.015 - embeddings: bert-base-chinese - GPU: AWS ml.p3.8xlarge SeqAL on different domain Ritter (SNS) Setup: - dataset: Ritter - model: Bi-LSTM CRF - total query data: 50% - epochs: 20 - batch_size: 32 - learning_rate: 0.015 - embeddings: bert-base-japanese - GPU: AWS g4dn.2xlarge Conclusion According to the performance of different sampling methods, MNLP is a proper baseline method. If we want to achieve high performance, we recommend MNLP+DS . Time cost According to the active learning cycle , we run experiments to check the time cost in each step. GPU model time cost in each step. The GPU model is Bi-LSTM CRF model. Below is the experiment setup. Setup Value Dataset Ontonotes 5.0 Model Bi-LSTM CRF Total query data 50% Iterations 25 Epochs in each iteration 20 Batchsize 32 Learning rate 0.015 Embeddings bert-base-uncased AWS machine AWS g4dn.2xlarge (0.752 USD/hour) GPU NVIDIA T4 CPU Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz Below is the percentage in each step. Below is the absolute value of time cost in each step. Below is the GPU model performance. CPU model time cost in each step. The CPU model is CRF model. Below is the experiment setup. Setup Value Dataset Ontonotes 5.0 Model CRF Total query data 50% Iterations 25 Epochs in each iteration 20 Batchsize 32 Learning rate 0.015 Embeddings bert-base-uncased AWS machine AWS c5.2xlarge (0.34 USD/hour) CPU Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz Below is the percentage in each step. Below is the absolute value of time cost in each step. Below is the CPU model performance. Conclusion Compare with the time cost and performance on GPU model and CPU model, we recommend use the CPU model. Because CPU model can decrease the time cost greatly only sacrificing a little performance. Another reason is that CPU model is cheaper than GPU mode. The price of CPU machine is about half price of GPU machine.","title":"Performance"},{"location":"performance/#seqal-performance-on-metrics-and-time-cost","text":"","title":"SeqAL Performance on Metrics and Time Cost"},{"location":"performance/#performance","text":"We will show the performance of SeqAL on different datasets with different language. Supervise means that training model on full data LC (Least confidence) and MNLP (Maximum Normalized Log-Probability) are query algorithm with different calculation on informativeness. Random means randomly query data without caring about informativeness. CS (Clustering Similarity) and DS (Distributed Similarity) are the diversity based sampling methods for NER We first train the model on seed data (2% of training data). Then we query 2\uff05 of training data in each iteration until 50% data are queried.","title":"Performance"},{"location":"performance/#seqal-on-different-languages","text":"Ontonotes 5.0 (EN) Setup: - dataset: CoNLL 2003(English) - model: Bi-LSTM CRF - total query data: 50% - epochs: 20 - batch_size: 32 - learning_rate: 0.015 - embeddings: bert-base-uncased - GPU: AWS p3.8xlarge BCCWJ (JP) Setup: - dataset: BCCWJ - model: Bi-LSTM CRF - total query data: 50% - epochs: 20 - batch_size: 32 - learning_rate: 0.015 - embeddings: bert-base-japanese - GPU: AWS p3.8xlarge People's Daily 2014 (CN) Setup: - dataset: People's Daily 2014 - model: Bi-LSTM CRF - total query data: 50% - epochs: 20 - batch_size: 32 - learning_rate: 0.015 - embeddings: bert-base-chinese - GPU: AWS ml.p3.8xlarge","title":"SeqAL on different languages"},{"location":"performance/#seqal-on-different-domain","text":"Ritter (SNS) Setup: - dataset: Ritter - model: Bi-LSTM CRF - total query data: 50% - epochs: 20 - batch_size: 32 - learning_rate: 0.015 - embeddings: bert-base-japanese - GPU: AWS g4dn.2xlarge","title":"SeqAL on different domain"},{"location":"performance/#conclusion","text":"According to the performance of different sampling methods, MNLP is a proper baseline method. If we want to achieve high performance, we recommend MNLP+DS .","title":"Conclusion"},{"location":"performance/#time-cost","text":"According to the active learning cycle , we run experiments to check the time cost in each step.","title":"Time cost"},{"location":"performance/#gpu-model-time-cost-in-each-step","text":"The GPU model is Bi-LSTM CRF model. Below is the experiment setup. Setup Value Dataset Ontonotes 5.0 Model Bi-LSTM CRF Total query data 50% Iterations 25 Epochs in each iteration 20 Batchsize 32 Learning rate 0.015 Embeddings bert-base-uncased AWS machine AWS g4dn.2xlarge (0.752 USD/hour) GPU NVIDIA T4 CPU Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz Below is the percentage in each step. Below is the absolute value of time cost in each step. Below is the GPU model performance.","title":"GPU model time cost in each step."},{"location":"performance/#cpu-model-time-cost-in-each-step","text":"The CPU model is CRF model. Below is the experiment setup. Setup Value Dataset Ontonotes 5.0 Model CRF Total query data 50% Iterations 25 Epochs in each iteration 20 Batchsize 32 Learning rate 0.015 Embeddings bert-base-uncased AWS machine AWS c5.2xlarge (0.34 USD/hour) CPU Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz Below is the percentage in each step. Below is the absolute value of time cost in each step. Below is the CPU model performance.","title":"CPU model time cost in each step."},{"location":"performance/#conclusion_1","text":"Compare with the time cost and performance on GPU model and CPU model, we recommend use the CPU model. Because CPU model can decrease the time cost greatly only sacrificing a little performance. Another reason is that CPU model is cheaper than GPU mode. The price of CPU machine is about half price of GPU machine.","title":"Conclusion"}]}